{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa596397",
   "metadata": {},
   "source": [
    "Consider a linear regression of the form (1) y = α + βx + u, with (y, x) both scalar random variables, where it is assumed that (a.i) E(u · x) = Eu = 0 and (a.ii) E(u2|x) = σ2.\n",
    "\n",
    "(1) The condition a.i is essentially untestable; explain why.\n",
    "\n",
    "Since u is not directly observable, we cannot measure its covariance with x, which is necessary to test whether E(u · x) = 0. We can only estimate the covariance between y and x, which is the sum of the covariance between u and x and the covariance between βx and x, but this does not allow us to separate the effect of the error term from the effect of the independent variable.\n",
    "\n",
    "Therefore, we cannot directly test whether a.i holds, but we can make assumptions or arguments about why it might be plausible. For example, if we believe that x is exogenously determined and does not depend on any unobservable factors that affect u, then we might argue that E(u · x) = 0 is a reasonable assumption. However, such assumptions or arguments cannot be empirically verified, and thus, the condition a.i is essentially untestable.\n",
    "\n",
    "(2) Breusch and Pagan (1979) argue that one can test a.ii via an auxiliary regression ˆu2 = c+dx+e, where the ˆu are the residuals from the first regression, and the test of a.ii then becomes a test of H0 : d = 0. Describe the logic of the test of a.ii. \n",
    "\n",
    " The logic of the test is as follows:\n",
    "\n",
    "Estimate the regression model (1) and obtain the residuals u_hat = y - α - βx.\n",
    "Regress the squared residuals, u_hat_squared, on the independent variable, x, and obtain the residuals of this regression, e.\n",
    "Test the null hypothesis that the slope coefficient in this auxiliary regression is zero, that is, H0: d = 0.\n",
    "\n",
    "If the null hypothesis is not rejected, then there is no evidence of conditional heteroskedasticity, and we can assume that the variance of the error term is constant conditional on x, and thus a.ii holds. On the other hand, if the null hypothesis is rejected, then there is evidence of conditional heteroskedasticity, and we cannot assume that the variance of the error term is constant conditional on x, and thus a.ii does not hold.\n",
    "\n",
    "The intuition behind this test is that if a.ii holds, then the squared residuals should not be related to the independent variable, x, and the slope coefficient in the auxiliary regression should be zero. However, if there is conditional heteroskedasticity, then the variance of the error term depends on x, and the squared residuals should be related to x, and the slope coefficient in the auxiliary regression should be nonzero.\n",
    "\n",
    "Therefore, the test proposed by Breusch and Pagan (1979) allows us to test for conditional heteroskedasticity in the linear regression model (1) by examining the relationship between the squared residuals and the independent variable, x.\n",
    "\n",
    "(3) Use the two conditions a.i and a.ii to construct a GMM version of the Breusch-Pagan test.\n",
    "\n",
    "Step 1: Identify the beliefs and assertions\n",
    "\n",
    "Step 2: Translate it into data\n",
    "E((y - α + βx). x) = 0; E((y - α + βx)^2 | x) = σ2\n",
    "\n",
    "Step 3: Going from expectations to yi, xi\n",
    "\n",
    "G_hat(N) = [(1/N) Σ((yi - α + βxi). xi)), (1/N) Σ(xi(yi - α + βxi)^2) - σ2)]\n",
    "\n",
    "GMM perfectly minimzies the objective function. Here, we want to construct a test to learn how well the minimization works. Right now, we have two unknowns and two equations. Only in the overidentifies case, there is a minimum. But we can also add more moments to create an overidentified model. For example, we can add another condition that E(u^2) = σ2. That is σ does not vary with x at all. \n",
    "\n",
    "(4) What can you say about the performance or relative merits of the Bruesch-Pagan test versus your GMM alternative?\n",
    "\n",
    "Good: Our test is a little more general than the Bruesch-Pagan test.\n",
    "Bad: Our test is checking for three things at once. One, is β a good estimate? Two, is σ2 a good estimate? Three, is the assumption that σ2 does not depend on x reasonable? If one of these conditions is off, for example is the last assumption does not hold, it can also affect how good my estimate of β is as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
