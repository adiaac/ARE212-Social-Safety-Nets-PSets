{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99664c96-cc67-454f-b8f0-b9ebff0cbf14",
   "metadata": {},
   "source": [
    "## Problem #4. Logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf1550-7d8d-446f-b4af-720b29af1a81",
   "metadata": {},
   "source": [
    "This problem is meant to help draw connections between GMM estimators and maximum likelihood estimators, with a particular focus on the 'logit' model.\n",
    "\n",
    "The development of a maximum likelihood estimator typically begins with an assumption that some random variable has a (conditional) distribution which is known up a k-vector of parameters $\\beta{}$. Consider the case in which we observe N independent realizations of a Bernoulli random variable Y, with $Pr(Y = 1|X) = \\sigma{}(\\beta{}^{T}X)$, and $Pr(Y = 0|X) = 1 - \\sigma{}(\\beta{^T}X).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0ff5e-1a07-48d6-a686-b957a7c7cec9",
   "metadata": {},
   "source": [
    "(1) Show that under this model $E(Y - \\sigma{(X\\beta{}}|X) = 0.$ Assume that $\\sigma{}$ is a known function, and use this fact to develop a GMM estimator of $\\beta{}$. Is your estimator just- or over-identified? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cad7522-31e7-4156-be76-b857ed9011ac",
   "metadata": {},
   "source": [
    "We want to show that $E(Y - \\sigma{(X\\beta{})}|X) = 0$. \n",
    "\n",
    "\n",
    "We can take the $\\sigma{X\\beta{}}$ out and we get $E(Y|X) - \\sigma{(X\\beta{})}$. \n",
    "\n",
    "The probability of the expectation of Y given X can be written as $P(Y=1|X)\\cdot{1} + P(Y=0|X)\\cdot{0} - \\sigma{(X\\beta{})}$. \n",
    "\n",
    "The probability of Y = 0 given X multipled by 0 cancels out which gives us, $\\sigma{(X\\beta{})}\\cdot{1} - \\sigma{(X\\beta{})} = 0$ and will subtract to 0.\n",
    "\n",
    "Since we have one moment condition and three parameters, our estimator is just-identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9399ae44-bda3-4a89-bfac-fdfad96297e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta estimates: [0.88955868 0.50268812 1.91392656]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "class GMMestimator:\n",
    "    def __init__(self, endog, exog, instrument):\n",
    "        self.endog = endog\n",
    "        self.exog = exog\n",
    "        self.instrument = instrument\n",
    "        self.nobs, self.nvar = self.exog.shape\n",
    "        self.null = np.zeros(self.exog.shape[1])\n",
    "    \n",
    "    def fit(self):\n",
    "        def moments(beta):\n",
    "            z = self.exog @ beta\n",
    "            eps_hat = self.endog - sigmoid(z)\n",
    "            return (self.instrument.T @ eps_hat, eps_hat * sigmoid(z) * (1 - sigmoid(z)))\n",
    "        \n",
    "        def gmm_objective(beta):\n",
    "            mom, _ = moments(beta)\n",
    "            return mom.T @ mom / self.nobs\n",
    "        \n",
    "        results = minimize(gmm_objective, self.null, method='BFGS', options={'disp': False})\n",
    "        self.beta = results.x\n",
    "        _, eps_hat = moments(self.beta)\n",
    "        self.sigma = np.cov(eps_hat.T, ddof=self.nvar)\n",
    "        return results\n",
    "        \n",
    "# Generate data\n",
    "np.random.seed(123)\n",
    "nobs = 1000\n",
    "beta = np.array([1, 2, 3])\n",
    "x = np.random.normal(size=(nobs, len(beta)))\n",
    "z = np.random.normal(size=(nobs, 3))\n",
    "z = np.column_stack((z, np.ones(nobs))) # add intercept to instrument\n",
    "eps = np.random.normal(scale=0.5, size=nobs)\n",
    "y = np.random.binomial(n=1, p=sigmoid(x @ beta), size=nobs)\n",
    "\n",
    "# Instantiate the GMM estimator class and fit the model\n",
    "gmm = GMMestimator(y, x, z)\n",
    "results = gmm.fit()\n",
    "\n",
    "# Print the results\n",
    "print(\"Beta estimates:\", results.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faee863-6fe0-4ba9-ac4b-06a07348ae28",
   "metadata": {},
   "source": [
    "(2) Show that the likelihood conditional on realizations of data (y, X) can be written as \n",
    "\n",
    "$$L(\\beta{}|y, X) = \\prod_{i=1}^N\\sigma(\\beta^T X_i)^{y_i}\\left(1-\\sigma(\\beta^T X_i)\\right)^{1-y_i}.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf5ca4f-a205-43f8-b334-83b11614eb72",
   "metadata": {},
   "source": [
    "For Bernoulli distribution $Pr(Y = 1|X) = \\sigma{(\\beta{^T}X})$ and $Pr(Y = 0|X) = 1-\\sigma{(\\beta{^T}X)}$, we can get the individual likelihood function, $L(\\beta{}|y_i,X_i) = \\sigma{(\\beta{^T}X_i)^y_i}{(1-\\sigma{(\\beta{^T}X_i}))^{1-y_i})}$, and obtain the likelihood function for all observation N by taking the product of all individual likelihoods:\n",
    "\n",
    "$$ = \\prod_{i=1}^N\\sigma(\\beta^T X_i)^{y_i}\\left(1-\\sigma(\\beta^T X_i)\\right)^{1-y_i} $$\n",
    "\n",
    "This gives us the likelihood function for logistic regression, which we can use to estimate the values of the regression coefficients that maximize the likelihood, given the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580887e2-322c-4f15-bcfd-9cdd3781741d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19428f2-b7d8-44ca-a4d6-d4e16e23d356",
   "metadata": {},
   "source": [
    "(3) To obtain the maximum likelihood estimator (MLE) one can chose b to maximize log L(b|y,X). When the likelihood is well-behaved, the MLE estimator satisfies the first order conditions (also called the \"scores\") from this maximization problem, in which case this is called a \"type I\" MLE. Let $\\sigma{(z)} = 1/(1+e^{-z})$ (this\n",
    "is sometimes called the logistic function, or the sigmoid function), and obtain the scores $S_n(b)$ for this estimation problem. Show that $ES_n(\\beta{}) = 0$. Demonstrate that these moment conditions can serve as the basis for a GMM estimator of $\\beta{}$, and compare this estimator to the GMM estimator you developed above. Which is more efficient, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6282d1d6-f639-4a4e-b581-47ddc38241c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True beta: [1 2 3]\n",
      "MLE: [1.58390826 1.79227628 3.10504348]\n",
      "GMM1: [1.6478029  1.56213396 2.84750278]\n",
      "GMM2: [1.56187198 1.80464808 3.09554716]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(123)\n",
    "n = 100\n",
    "p = 3\n",
    "X = np.random.normal(size=(n,p))\n",
    "beta_true = np.array([1,2,3])\n",
    "y = np.random.binomial(n=1, p=sigmoid(np.dot(X, beta_true)))\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# Define the log-likelihood function\n",
    "def log_likelihood(beta, y, X):\n",
    "    z = np.dot(X, beta)\n",
    "    return np.sum(y*np.log(sigmoid(z)) + (1-y)*np.log(1-sigmoid(z)))\n",
    "\n",
    "def scores(beta, y, X):\n",
    "    z = np.dot(X, beta)\n",
    "    return (sigmoid(z).reshape(-1, 1) - y.reshape(-1, 1)) * X\n",
    "\n",
    "# Define the weight matrix function\n",
    "def weight_matrix(S):\n",
    "    W = np.dot(S.T, S)/n\n",
    "    return W\n",
    "\n",
    "# Define the moment condition function\n",
    "def moment_condition(beta, y, X):\n",
    "    z = np.dot(X, beta)\n",
    "    return X*(y - sigmoid(z))[:,np.newaxis]\n",
    "\n",
    "# Calculate the MLE\n",
    "result = minimize(lambda beta: -log_likelihood(beta, y, X), np.zeros(p))\n",
    "beta_mle = result.x\n",
    "\n",
    "# Calculate the scores and GMM estimator\n",
    "S = scores(beta_mle, y, X)\n",
    "W = weight_matrix(S)\n",
    "beta_gmm1 = beta_mle + np.dot(np.linalg.inv(W), np.dot(S.T, y - sigmoid(np.dot(X, beta_mle))))/n\n",
    "\n",
    "# Calculate the moment conditions and second-step GMM estimator\n",
    "Z = moment_condition(beta_gmm1, y, X)\n",
    "G = np.dot(Z.T, Z)/n\n",
    "beta_gmm2 = beta_gmm1 + np.dot(np.linalg.inv(G), np.dot(Z.T, y - sigmoid(np.dot(X, beta_gmm1))))/n\n",
    "\n",
    "\n",
    "print(\"True beta:\", beta_true)\n",
    "print(\"MLE:\", beta_mle)\n",
    "print(\"GMM1:\", beta_gmm1)\n",
    "print(\"GMM2:\", beta_gmm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c0905-954f-4703-8ffc-dfe26c049f83",
   "metadata": {},
   "source": [
    "This GMM estimator is more efficient than the above estimator, since it can make use of additional information in the moment conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
